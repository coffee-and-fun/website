---
new: false
submit: false
footer: true
header: true
layout: templates/post.liquid
title: Scraping 20,000 Poshmark Listings (For Real This Time)
description: Back last week, I wrote a blog post titled How I Scraped 20,000 Poshmark Listings Using Node.js and Puppeteer. The idea seemed solid, scroll through a Poshmark closet, grab every listing URL, and scrape each listing one by one. But in reality? The script crashed constantly. It would hang, freeze, or get stuck on a single listing. Chrome would balloon in memory, and Puppeteer would throw vague timeout errors like
keywords: "ethical scraping, Poshmark backup, Node.js Puppeteer tutorial, Poshmark automation, online listing backup, ethical scraping guide, Puppeteer project, ecommerce recovery, scraping listings Node.js"
url: blog/ethically-scrape-poshmark-listings-node-puppeteer-update/
isBlog: true
blog_cat: Usecase
cardTitle: Scraping 20,000 Poshmark Listings (For Real This Time)
blog_snip: Back last week, I wrote a blog post titled How I Scraped 20,000 Poshmark Listings Using Node.js and Puppeteer. The idea seemed solid, scroll through a Poshmark closet, grab every listing URL, and scrape each listing one by one. But in reality? The script crashed constantly. It would hang, freeze, or get stuck on a single listing. Chrome would balloon in memory, and Puppeteer would throw vague timeout errors like
name: Robert James Gabriel
img: /assets/images/blog/ethically-scrape-poshmark-listings-node-puppeteer.png
date: 2025-03-28T00:00:00.000Z
time: 6 min
tags:
  - scraping
  - guide
  - puppeteer
---

# Scraping 20,000 Poshmark Listings (For Real This Time)

Back last week, I wrote a blog post titled *[How I Scraped 20,000 Poshmark Listings Using Node.js and Puppeteer](blog/ethically-scrape-poshmark-listings-node-puppeteer)*. The idea seemed solid: scroll through a Poshmark closet, grab every listing URL, and scrape each listing one by one.

But in reality? The script crashed constantly.

It would hang, freeze, or get stuck on a single listing. Chrome would balloon in memory, and Puppeteer would throw vague timeout errors like:

> `Runtime.callFunctionOn timed out. Increase the 'protocolTimeout' setting in launch/connect calls...`

After way too many failed runs, I finally figured out why â€” and how to actually fix it. This post is the follow-up to that original blog, walking through what went wrong and how I made it right.

---

## ğŸ§¨ What Went Wrong

Hereâ€™s how my original scraper worked:

```js
const browser = await puppeteer.launch();
const page = await browser.newPage();

for (let url of allListingUrls) {
  await page.goto(url);
  const images = await page.$$eval(...);
  await saveImages(images);
}
```


It looked simple, but under the hood, there were big issues:

- One long browser session for 20,000 listings = eventual crash

- No timeout protection meant a single slow page could freeze everything

- No resume support, so if it crashed at item #1375, Iâ€™d have to start all over

- No skip logic, so re-running would re-scrape everything


## âœ… The New Approach: Batch It, Restart It, Skip It
I rewrote the entire scraper with resilience in mind. The result is a scraper that:

- Splits all listings into **batches of 10**
- Starts a **fresh Puppeteer browser** for each batch
- **Skips already-scraped listings**
- Handles **timeouts gracefully**
- **Saves progress** with a simple text file so I can resume anytime


### ğŸ” Batching the Work

``` js
const BATCH_SIZE = 10;
const totalBatches = Math.ceil(allListingUrls.length / BATCH_SIZE);

for (let b = 0; b < totalBatches; b++) {
  const batch = allListingUrls.slice(b * BATCH_SIZE, (b + 1) * BATCH_SIZE);
  // process this batch
}
```

This keeps memory usage low and makes failures isolated and easy to recover from.


--- 


### ğŸ§¼ Fresh Browser Per Batch

```js

const { browser, page } = await createBrowser();
// ... scrape listings ...
await browser.close();

```

No more memory leaks, zombie tabs, or weird state.

---


### â±ï¸ Timeout Safety with Promise.race

```js

await Promise.race([
  page.goto(url, { timeout: 3000 }),
  sleep(4000).then(() => { throw new Error("Timeout!") })
]);
```

If a page doesn't load quickly, we kill it and move on.


---


### ğŸ§  Skip Already-Scraped Listings

```js

if (fs.existsSync(folderPath)) {
  console.log("Already scraped, skipping.");
  continue;
}

```

Letâ€™s me run the scraper over and over again without repeating work.


### ğŸ’¾ Save and Resume with last_batch.txt

```js
writeLastBatchIndex(currentBatchIndex);
```

On each batch, the script saves its progress. So even if I kill it, crash it, or lose power â€” itâ€™ll pick right back up where it left off.

--- 

## ğŸ¤¯ The Difference

| Feature                         | Old Script âŒ       | New Script âœ…          |
|--------------------------------|---------------------|------------------------|
| One browser for all listings   | Yes                 | No â€” fresh browser per batch |
| Timeout handling               | No                  | Yes â€” fast fail and move on |
| Resume support                 | No                  | Yes â€” `last_batch.txt` |
| Re-scrape protection           | No                  | Yes â€” skips done folders |
| Batching                       | No (1 big loop)     | Yes â€” smaller and stable |
| Reliable over 1,000+ listings  | Unstable            | Solid and scalable ğŸš€  |

---

## ğŸš€ Final Thoughts

The first version was a cool proof-of-concept, but the new version is a production-grade scraper I can actually trust. Itâ€™s crash-safe, restartable, and skips already-processed listings.

If you're trying to scrape large datasets with Puppeteer and running into crashes, batching and timeout management will save your sanity.

Want the final working code? [DM me on Twitter](https://www.twitter.com/bycoffeeanfun) or [email me](mailto:hello@robertgabriel.ninja), happy to share!
